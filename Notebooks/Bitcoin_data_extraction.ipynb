{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blockchain-Framework/bitcoin-anomaly-analysis/blob/develop/Notebooks/Bitcoin_data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl3vG3GdAXCV",
        "outputId": "0728eed0-7802-4888-ac2b-3306b3ba9525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.6/461.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.16.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4tGHSo4Aklc",
        "outputId": "0f79cc00-eb1a-4f07-ea51-23412a4563a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [1 InRelease 0 B/3,626\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [Connected to ppa.laun\r                                                                                                    \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [637 kB]\n",
            "Hit:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n",
            "Fetched 6,147 kB in 2s (2,987 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 26 not upgraded.\n",
            "Need to get 26.4 MB of archives.\n",
            "After this operation, 116 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.11 [78.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.11 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.58+22.04.1 [23.8 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.4 [2,978 B]\n",
            "Fetched 26.4 MB in 1s (21.2 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.11) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 121866 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.58+22.04.1_amd64.deb ...\n",
            "Unpacking snapd (2.58+22.04.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.11) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.58+22.04.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.aa-prompt-listener.service → /lib/systemd/system/snapd.aa-prompt-listener.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 122099 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.4_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.4) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.4) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.11) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.16.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.23.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QH67E-8BAnRu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q47WYB6aAu65"
      },
      "source": [
        "## Extract transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzGyYKZMAsGh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def process_files(start_date, end_date):\n",
        "    base_url = \"http://blockdata.loyce.club\"\n",
        "    download_path = \"/content/drive/MyDrive/bitcoin\"  # Replace with the path to your directory\n",
        "    # Convert date strings to datetime objects\n",
        "    start_date = datetime.strptime(start_date, '%Y%m%d')\n",
        "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
        "\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        date_str = current_date.strftime('%Y%m%d')\n",
        "\n",
        "        # Define URLs for the input transactions and transactions data\n",
        "        input_url = f\"{base_url}/inputs/blockchair_bitcoin_inputs_{date_str}.tsv.gz\"\n",
        "        trx_url = f\"{base_url}/transactions/blockchair_bitcoin_transactions_{date_str}.tsv.gz\"\n",
        "\n",
        "        # Download the data\n",
        "        df_input = pd.read_csv(input_url, compression='gzip', sep='\\t')\n",
        "        df_trx = pd.read_csv(trx_url, compression='gzip', sep='\\t')\n",
        "\n",
        "        # # Filter and rename columns in the input transactions data\n",
        "        wanted = [\"recipient\", \"spending_transaction_hash\", \"time\", \"value\"]\n",
        "        df_input_filtered = df_input[wanted]\n",
        "        df_input_filtered = df_input_filtered.rename(columns={\"recipient\": \"account\", \"spending_transaction_hash\": \"hash\"})\n",
        "\n",
        "        # # Merge the dataframes\n",
        "        # df_merged = pd.merge(df_input_filtered, df_trx, on='hash', how='inner')\n",
        "\n",
        "        # Define the save paths and ensure the directories exist\n",
        "        input_folder = os.path.join(download_path, 'inputs')\n",
        "        trx_folder = os.path.join(download_path,  'transactions')\n",
        "        # merged_folder = os.path.join(download_path, 'merged')\n",
        "\n",
        "        # if not os.path.exists(trx_folder):\n",
        "        #         os.makedirs(trx_folder)\n",
        "\n",
        "        for folder in [input_folder, trx_folder]:\n",
        "            if not os.path.exists(folder):\n",
        "                os.makedirs(folder)\n",
        "\n",
        "        df_input.to_csv(os.path.join(input_folder, f\"inputs_{date_str}.tsv.gz\"), index=False, sep='\\t', compression='gzip')\n",
        "        df_trx.to_csv(os.path.join(trx_folder, f\"transactions_{date_str}.tsv.gz\"), index=False, sep='\\t', compression='gzip')\n",
        "        # df_merged.to_csv(os.path.join(merged_folder, f\"merged_{date_str}.tsv.gz\"), index=False, sep='\\t', compression='gzip')\n",
        "\n",
        "        print(f\"Processed and saved data for {date_str}\")\n",
        "\n",
        "        # Move to the next date\n",
        "        current_date += timedelta(days=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRRSAOgRARX",
        "outputId": "3772328e-e815-4680-ac17-ba41dfe44ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9HQgc57CUed",
        "outputId": "ca2784af-6b4e-41b7-eb36-7d56927c35da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed and saved data for 20200101\n",
            "Processed and saved data for 20200102\n",
            "Processed and saved data for 20200103\n",
            "Processed and saved data for 20200104\n",
            "Processed and saved data for 20200105\n",
            "Processed and saved data for 20200106\n",
            "Processed and saved data for 20200107\n",
            "Processed and saved data for 20200108\n",
            "Processed and saved data for 20200109\n",
            "Processed and saved data for 20200110\n",
            "Processed and saved data for 20200111\n",
            "Processed and saved data for 20200112\n",
            "Processed and saved data for 20200113\n",
            "Processed and saved data for 20200114\n",
            "Processed and saved data for 20200115\n",
            "Processed and saved data for 20200116\n",
            "Processed and saved data for 20200117\n",
            "Processed and saved data for 20200118\n",
            "Processed and saved data for 20200119\n",
            "Processed and saved data for 20200120\n",
            "Processed and saved data for 20200121\n",
            "Processed and saved data for 20200122\n",
            "Processed and saved data for 20200123\n",
            "Processed and saved data for 20200124\n",
            "Processed and saved data for 20200125\n",
            "Processed and saved data for 20200126\n",
            "Processed and saved data for 20200127\n",
            "Processed and saved data for 20200128\n",
            "Processed and saved data for 20200129\n",
            "Processed and saved data for 20200130\n",
            "Processed and saved data for 20200131\n",
            "Processed and saved data for 20200201\n",
            "Processed and saved data for 20200202\n",
            "Processed and saved data for 20200203\n",
            "Processed and saved data for 20200204\n",
            "Processed and saved data for 20200205\n",
            "Processed and saved data for 20200206\n",
            "Processed and saved data for 20200207\n",
            "Processed and saved data for 20200208\n",
            "Processed and saved data for 20200209\n",
            "Processed and saved data for 20200210\n",
            "Processed and saved data for 20200211\n",
            "Processed and saved data for 20200212\n",
            "Processed and saved data for 20200213\n",
            "Processed and saved data for 20200214\n",
            "Processed and saved data for 20200215\n",
            "Processed and saved data for 20200216\n",
            "Processed and saved data for 20200217\n",
            "Processed and saved data for 20200218\n",
            "Processed and saved data for 20200219\n",
            "Processed and saved data for 20200220\n",
            "Processed and saved data for 20200221\n",
            "Processed and saved data for 20200222\n",
            "Processed and saved data for 20200223\n",
            "Processed and saved data for 20200224\n",
            "Processed and saved data for 20200225\n",
            "Processed and saved data for 20200226\n",
            "Processed and saved data for 20200227\n",
            "Processed and saved data for 20200228\n",
            "Processed and saved data for 20200229\n",
            "Processed and saved data for 20200301\n",
            "Processed and saved data for 20200302\n",
            "Processed and saved data for 20200303\n",
            "Processed and saved data for 20200304\n",
            "Processed and saved data for 20200305\n",
            "Processed and saved data for 20200306\n",
            "Processed and saved data for 20200307\n",
            "Processed and saved data for 20200308\n",
            "Processed and saved data for 20200309\n",
            "Processed and saved data for 20200310\n",
            "Processed and saved data for 20200311\n",
            "Processed and saved data for 20200312\n",
            "Processed and saved data for 20200313\n",
            "Processed and saved data for 20200314\n",
            "Processed and saved data for 20200315\n",
            "Processed and saved data for 20200316\n",
            "Processed and saved data for 20200317\n",
            "Processed and saved data for 20200318\n",
            "Processed and saved data for 20200319\n",
            "Processed and saved data for 20200320\n",
            "Processed and saved data for 20200321\n",
            "Processed and saved data for 20200322\n",
            "Processed and saved data for 20200323\n",
            "Processed and saved data for 20200324\n",
            "Processed and saved data for 20200325\n",
            "Processed and saved data for 20200326\n",
            "Processed and saved data for 20200327\n",
            "Processed and saved data for 20200328\n",
            "Processed and saved data for 20200329\n",
            "Processed and saved data for 20200330\n",
            "Processed and saved data for 20200331\n"
          ]
        }
      ],
      "source": [
        "process_files('20200101', '20200331')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EwxUCS4Ulv41"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "\n",
        "def get_account_time_value(start_date, end_date):\n",
        "    base_url = \"http://blockdata.loyce.club\"\n",
        "    download_path = \"/content/drive/MyDrive/Colab Notebooks/FYP/data\"  # Replace with the path to your directory\n",
        "    # Convert date strings to datetime objects\n",
        "    start_date = datetime.strptime(start_date, '%Y%m%d')\n",
        "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "\n",
        "      babd = pd.read_csv(\"/content/drive/MyDrive/combined_babd_sample.csv\")\n",
        "      babd['account'] = babd['account'].astype('category')\n",
        "\n",
        "      current_date = start_date\n",
        "      while current_date <= end_date:\n",
        "          date_str = current_date.strftime('%Y%m%d')\n",
        "\n",
        "          # Define URLs for the input transactions and transactions data\n",
        "          input_url = f\"{base_url}/inputs/blockchair_bitcoin_inputs_{date_str}.tsv.gz\"\n",
        "          trx_url = f\"{base_url}/transactions/blockchair_bitcoin_transactions_{date_str}.tsv.gz\"\n",
        "          output_url = f\"{base_url}/outputs/blockchair_bitcoin_outputs_{date_str}.tsv.gz\"\n",
        "\n",
        "          # Download the data\n",
        "          df_input = pd.read_csv(input_url, compression='gzip', sep='\\t')\n",
        "          df_trx = pd.read_csv(trx_url, compression='gzip', sep='\\t')\n",
        "          df_output = pd.read_csv(output_url, compression='gzip', sep='\\t')\n",
        "\n",
        "          # Filter and rename columns in the input transactions data\n",
        "          wanted_input = [\"recipient\", \"spending_transaction_hash\", \"time\", \"value\", \"value_usd\", \"spending_value_usd\"]\n",
        "          wanted_output = [\"recipient\", \"transaction_hash\", \"time\", \"value\", \"value_usd\"]\n",
        "          wanted_transaction = [\"hash\", \"time\", \"is_coinbase\", \"block_id\"]\n",
        "          df_input_filtered = df_input[wanted_input]\n",
        "          df_input_filtered = df_input_filtered.rename(columns={\"recipient\": \"account\", \"spending_transaction_hash\": \"hash\"})\n",
        "\n",
        "          df_output_filtered = df_output[wanted_output]\n",
        "          df_output_filtered = df_output_filtered.rename(columns={\"recipient\": \"account\", \"transaction_hash\": \"hash\"})\n",
        "\n",
        "          df_trx_filtered = df_trx[wanted_transaction]\n",
        "\n",
        "          df_input_filtered['account'] = df_input_filtered['account'].astype('category')\n",
        "          df_input_filtered['hash'] = df_input_filtered['hash'].astype('category')\n",
        "\n",
        "          df_output_filtered['account'] = df_output_filtered['account'].astype('category')\n",
        "          df_output_filtered['hash'] = df_output_filtered['hash'].astype('category')\n",
        "\n",
        "          df_trx_filtered['hash'] = df_trx_filtered['hash'].astype('category')\n",
        "\n",
        "\n",
        "          # Merge the dataframes\n",
        "          df_merged_input = pd.merge(df_input_filtered, df_trx_filtered, on='hash', how='inner')\n",
        "          df_merged_output = pd.merge(df_output_filtered, df_trx_filtered, on='hash', how='inner')\n",
        "\n",
        "          # Convert time_y to datetime\n",
        "          df_merged_input['time'] = pd.to_datetime(df_merged_input['time_y'])\n",
        "          df_merged_output['time'] = pd.to_datetime(df_merged_output['time_y'])\n",
        "\n",
        "          # Convert value to int (or use appropriate conversion based on the nature of your data)\n",
        "          df_merged_input['value'] = df_merged_input['value'].astype(int)\n",
        "          df_merged_output['value'] = df_merged_output['value'].astype(int)\n",
        "\n",
        "          df_filtered_input = df_merged_input[df_merged_input['account'].isin(babd['account'])]\n",
        "          df_filtered_output = df_merged_output[df_merged_output['account'].isin(babd['account'])]\n",
        "\n",
        "          # Convert the datetime column to UNIX timestamp\n",
        "          # df_merged['time'] = df_merged['time'].astype(int) / 10**9\n",
        "\n",
        "          # Filter necessary columns\n",
        "          # df_filtered = df_merged[['account', 'time', 'value']]\n",
        "\n",
        "          # Define the save paths and ensure the directories exist\n",
        "          data_folder_input = os.path.join(download_path, 'input acc details csv')\n",
        "\n",
        "          if not os.path.exists(data_folder_input):\n",
        "              os.makedirs(data_folder_input)\n",
        "\n",
        "          data_folder_output = os.path.join(download_path, 'output acc details csv')\n",
        "\n",
        "          if not os.path.exists(data_folder_output):\n",
        "              os.makedirs(data_folder_output)\n",
        "\n",
        "          df_filtered_input.to_csv(os.path.join(data_folder_input, f\"input_acc_details_{date_str}.tsv.gz\"), index=False, sep='\\t', compression='gzip')\n",
        "          df_filtered_output.to_csv(os.path.join(data_folder_output, f\"output_acc_details_{date_str}.tsv.gz\"), index=False, sep='\\t', compression='gzip')\n",
        "\n",
        "          print(f\"Processed and saved data for {date_str}\")\n",
        "\n",
        "          # Move to the next date\n",
        "          current_date += timedelta(days=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIcYG6dhlzK8",
        "outputId": "41d47627-28d3-454d-ddd6-d2226fb71c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for 20180101\n",
            "Processed and saved data for 20180102\n",
            "Processed and saved data for 20180103\n",
            "Processed and saved data for 20180104\n",
            "Processed and saved data for 20180105\n",
            "Processed and saved data for 20180106\n",
            "Processed and saved data for 20180107\n",
            "Processed and saved data for 20180108\n",
            "Processed and saved data for 20180109\n",
            "Processed and saved data for 20180110\n",
            "Processed and saved data for 20180111\n",
            "Processed and saved data for 20180112\n",
            "Processed and saved data for 20180113\n",
            "Processed and saved data for 20180114\n",
            "Processed and saved data for 20180115\n",
            "Processed and saved data for 20180116\n",
            "Processed and saved data for 20180117\n",
            "Processed and saved data for 20180118\n",
            "Processed and saved data for 20180119\n",
            "Processed and saved data for 20180120\n",
            "Processed and saved data for 20180121\n",
            "Processed and saved data for 20180122\n",
            "Processed and saved data for 20180123\n",
            "Processed and saved data for 20180124\n",
            "Processed and saved data for 20180125\n",
            "Processed and saved data for 20180126\n",
            "Processed and saved data for 20180127\n",
            "Processed and saved data for 20180128\n",
            "Processed and saved data for 20180129\n",
            "Processed and saved data for 20180130\n",
            "Processed and saved data for 20180131\n",
            "Processed and saved data for 20180201\n",
            "Processed and saved data for 20180202\n",
            "Processed and saved data for 20180203\n",
            "Processed and saved data for 20180204\n",
            "Processed and saved data for 20180205\n",
            "Processed and saved data for 20180206\n",
            "Processed and saved data for 20180207\n",
            "Processed and saved data for 20180208\n",
            "Processed and saved data for 20180209\n",
            "Processed and saved data for 20180210\n",
            "Processed and saved data for 20180211\n",
            "Processed and saved data for 20180212\n",
            "Processed and saved data for 20180213\n",
            "Processed and saved data for 20180214\n",
            "Processed and saved data for 20180215\n",
            "Processed and saved data for 20180216\n",
            "Processed and saved data for 20180217\n",
            "Processed and saved data for 20180218\n",
            "Processed and saved data for 20180219\n",
            "Processed and saved data for 20180220\n",
            "Processed and saved data for 20180221\n",
            "Processed and saved data for 20180222\n",
            "Processed and saved data for 20180223\n",
            "Processed and saved data for 20180224\n",
            "Processed and saved data for 20180225\n",
            "Processed and saved data for 20180226\n",
            "Processed and saved data for 20180227\n",
            "Processed and saved data for 20180228\n",
            "Processed and saved data for 20180301\n",
            "Processed and saved data for 20180302\n",
            "Processed and saved data for 20180303\n",
            "Processed and saved data for 20180304\n",
            "Processed and saved data for 20180305\n",
            "Processed and saved data for 20180306\n",
            "Processed and saved data for 20180307\n",
            "Processed and saved data for 20180308\n",
            "Processed and saved data for 20180309\n",
            "Processed and saved data for 20180310\n",
            "Processed and saved data for 20180311\n",
            "Processed and saved data for 20180312\n",
            "Processed and saved data for 20180313\n",
            "Processed and saved data for 20180314\n",
            "Processed and saved data for 20180315\n",
            "Processed and saved data for 20180316\n",
            "Processed and saved data for 20180317\n",
            "Processed and saved data for 20180318\n",
            "Processed and saved data for 20180319\n",
            "Processed and saved data for 20180320\n",
            "Processed and saved data for 20180321\n",
            "Processed and saved data for 20180322\n",
            "Processed and saved data for 20180323\n",
            "Processed and saved data for 20180324\n",
            "Processed and saved data for 20180325\n",
            "Processed and saved data for 20180326\n",
            "Processed and saved data for 20180327\n",
            "Processed and saved data for 20180328\n",
            "Processed and saved data for 20180329\n",
            "Processed and saved data for 20180330\n",
            "Processed and saved data for 20180331\n",
            "Processed and saved data for 20180401\n",
            "Processed and saved data for 20180402\n",
            "Processed and saved data for 20180403\n",
            "Processed and saved data for 20180404\n",
            "Processed and saved data for 20180405\n",
            "Processed and saved data for 20180406\n",
            "Processed and saved data for 20180407\n",
            "Processed and saved data for 20180408\n",
            "Processed and saved data for 20180409\n",
            "Processed and saved data for 20180410\n",
            "Processed and saved data for 20180411\n",
            "Processed and saved data for 20180412\n",
            "Processed and saved data for 20180413\n",
            "Processed and saved data for 20180414\n",
            "Processed and saved data for 20180415\n",
            "Processed and saved data for 20180416\n",
            "Processed and saved data for 20180417\n",
            "Processed and saved data for 20180418\n",
            "Processed and saved data for 20180419\n",
            "Processed and saved data for 20180420\n",
            "Processed and saved data for 20180421\n",
            "Processed and saved data for 20180422\n",
            "Processed and saved data for 20180423\n",
            "Processed and saved data for 20180424\n",
            "Processed and saved data for 20180425\n",
            "Processed and saved data for 20180426\n",
            "Processed and saved data for 20180427\n",
            "Processed and saved data for 20180428\n",
            "Processed and saved data for 20180429\n",
            "Processed and saved data for 20180430\n",
            "Processed and saved data for 20180501\n",
            "Processed and saved data for 20180502\n",
            "Processed and saved data for 20180503\n",
            "Processed and saved data for 20180504\n",
            "Processed and saved data for 20180505\n",
            "Processed and saved data for 20180506\n",
            "Processed and saved data for 20180507\n",
            "Processed and saved data for 20180508\n",
            "Processed and saved data for 20180509\n",
            "Processed and saved data for 20180510\n",
            "Processed and saved data for 20180511\n",
            "Processed and saved data for 20180512\n",
            "Processed and saved data for 20180513\n",
            "Processed and saved data for 20180514\n",
            "Processed and saved data for 20180515\n",
            "Processed and saved data for 20180516\n",
            "Processed and saved data for 20180517\n",
            "Processed and saved data for 20180518\n",
            "Processed and saved data for 20180519\n",
            "Processed and saved data for 20180520\n",
            "Processed and saved data for 20180521\n",
            "Processed and saved data for 20180522\n",
            "Processed and saved data for 20180523\n",
            "Processed and saved data for 20180524\n",
            "Processed and saved data for 20180525\n",
            "Processed and saved data for 20180526\n",
            "Processed and saved data for 20180527\n",
            "Processed and saved data for 20180528\n",
            "Processed and saved data for 20180529\n",
            "Processed and saved data for 20180530\n",
            "Processed and saved data for 20180531\n",
            "Processed and saved data for 20180601\n",
            "Processed and saved data for 20180602\n",
            "Processed and saved data for 20180603\n",
            "Processed and saved data for 20180604\n",
            "Processed and saved data for 20180605\n",
            "Processed and saved data for 20180606\n",
            "Processed and saved data for 20180607\n",
            "Processed and saved data for 20180608\n",
            "Processed and saved data for 20180609\n",
            "Processed and saved data for 20180610\n",
            "Processed and saved data for 20180611\n",
            "Processed and saved data for 20180612\n",
            "Processed and saved data for 20180613\n",
            "Processed and saved data for 20180614\n",
            "Processed and saved data for 20180615\n",
            "Processed and saved data for 20180616\n",
            "Processed and saved data for 20180617\n",
            "Processed and saved data for 20180618\n",
            "Processed and saved data for 20180619\n",
            "Processed and saved data for 20180620\n",
            "Processed and saved data for 20180621\n",
            "Processed and saved data for 20180622\n",
            "Processed and saved data for 20180623\n",
            "Processed and saved data for 20180624\n",
            "Processed and saved data for 20180625\n",
            "Processed and saved data for 20180626\n",
            "Processed and saved data for 20180627\n",
            "Processed and saved data for 20180628\n",
            "Processed and saved data for 20180629\n",
            "Processed and saved data for 20180630\n",
            "Processed and saved data for 20180701\n",
            "Processed and saved data for 20180702\n",
            "Processed and saved data for 20180703\n",
            "Processed and saved data for 20180704\n",
            "Processed and saved data for 20180705\n",
            "Processed and saved data for 20180706\n",
            "Processed and saved data for 20180707\n",
            "Processed and saved data for 20180708\n",
            "Processed and saved data for 20180709\n",
            "Processed and saved data for 20180710\n",
            "Processed and saved data for 20180711\n",
            "Processed and saved data for 20180712\n",
            "Processed and saved data for 20180713\n",
            "Processed and saved data for 20180714\n",
            "Processed and saved data for 20180715\n",
            "Processed and saved data for 20180716\n",
            "Processed and saved data for 20180717\n",
            "Processed and saved data for 20180718\n",
            "Processed and saved data for 20180719\n",
            "Processed and saved data for 20180720\n",
            "Processed and saved data for 20180721\n",
            "Processed and saved data for 20180722\n",
            "Processed and saved data for 20180723\n",
            "Processed and saved data for 20180724\n",
            "Processed and saved data for 20180725\n",
            "Processed and saved data for 20180726\n",
            "Processed and saved data for 20180727\n",
            "Processed and saved data for 20180728\n",
            "Processed and saved data for 20180729\n",
            "Processed and saved data for 20180730\n",
            "Processed and saved data for 20180731\n",
            "Processed and saved data for 20180801\n",
            "Processed and saved data for 20180802\n",
            "Processed and saved data for 20180803\n",
            "Processed and saved data for 20180804\n",
            "Processed and saved data for 20180805\n",
            "Processed and saved data for 20180806\n",
            "Processed and saved data for 20180807\n",
            "Processed and saved data for 20180808\n",
            "Processed and saved data for 20180809\n",
            "Processed and saved data for 20180810\n",
            "Processed and saved data for 20180811\n",
            "Processed and saved data for 20180812\n",
            "Processed and saved data for 20180813\n",
            "Processed and saved data for 20180814\n",
            "Processed and saved data for 20180815\n",
            "Processed and saved data for 20180816\n",
            "Processed and saved data for 20180817\n",
            "Processed and saved data for 20180818\n",
            "Processed and saved data for 20180819\n",
            "Processed and saved data for 20180820\n",
            "Processed and saved data for 20180821\n",
            "Processed and saved data for 20180822\n",
            "Processed and saved data for 20180823\n",
            "Processed and saved data for 20180824\n",
            "Processed and saved data for 20180825\n",
            "Processed and saved data for 20180826\n",
            "Processed and saved data for 20180827\n",
            "Processed and saved data for 20180828\n",
            "Processed and saved data for 20180829\n",
            "Processed and saved data for 20180830\n",
            "Processed and saved data for 20180831\n",
            "Processed and saved data for 20180901\n",
            "Processed and saved data for 20180902\n",
            "Processed and saved data for 20180903\n",
            "Processed and saved data for 20180904\n",
            "Processed and saved data for 20180905\n",
            "Processed and saved data for 20180906\n",
            "Processed and saved data for 20180907\n",
            "Processed and saved data for 20180908\n",
            "Processed and saved data for 20180909\n",
            "Processed and saved data for 20180910\n",
            "Processed and saved data for 20180911\n",
            "Processed and saved data for 20180912\n",
            "Processed and saved data for 20180913\n",
            "Processed and saved data for 20180914\n",
            "Processed and saved data for 20180915\n",
            "Processed and saved data for 20180916\n",
            "Processed and saved data for 20180917\n",
            "Processed and saved data for 20180918\n",
            "Processed and saved data for 20180919\n",
            "Processed and saved data for 20180920\n",
            "Processed and saved data for 20180921\n",
            "Processed and saved data for 20180922\n",
            "Processed and saved data for 20180923\n",
            "Processed and saved data for 20180924\n",
            "Processed and saved data for 20180925\n",
            "Processed and saved data for 20180926\n",
            "Processed and saved data for 20180927\n",
            "Processed and saved data for 20180928\n",
            "Processed and saved data for 20180929\n",
            "Processed and saved data for 20180930\n",
            "Processed and saved data for 20181001\n",
            "Processed and saved data for 20181002\n",
            "Processed and saved data for 20181003\n",
            "Processed and saved data for 20181004\n",
            "Processed and saved data for 20181005\n",
            "Processed and saved data for 20181006\n",
            "Processed and saved data for 20181007\n",
            "Processed and saved data for 20181008\n",
            "Processed and saved data for 20181009\n",
            "Processed and saved data for 20181010\n",
            "Processed and saved data for 20181011\n",
            "Processed and saved data for 20181012\n",
            "Processed and saved data for 20181013\n",
            "Processed and saved data for 20181014\n",
            "Processed and saved data for 20181015\n",
            "Processed and saved data for 20181016\n",
            "Processed and saved data for 20181017\n",
            "Processed and saved data for 20181018\n",
            "Processed and saved data for 20181019\n",
            "Processed and saved data for 20181020\n",
            "Processed and saved data for 20181021\n",
            "Processed and saved data for 20181022\n",
            "Processed and saved data for 20181023\n",
            "Processed and saved data for 20181024\n",
            "Processed and saved data for 20181025\n",
            "Processed and saved data for 20181026\n",
            "Processed and saved data for 20181027\n",
            "Processed and saved data for 20181028\n",
            "Processed and saved data for 20181029\n",
            "Processed and saved data for 20181030\n",
            "Processed and saved data for 20181031\n",
            "Processed and saved data for 20181101\n",
            "Processed and saved data for 20181102\n",
            "Processed and saved data for 20181103\n",
            "Processed and saved data for 20181104\n",
            "Processed and saved data for 20181105\n",
            "Processed and saved data for 20181106\n",
            "Processed and saved data for 20181107\n",
            "Processed and saved data for 20181108\n",
            "Processed and saved data for 20181109\n",
            "Processed and saved data for 20181110\n",
            "Processed and saved data for 20181111\n",
            "Processed and saved data for 20181112\n",
            "Processed and saved data for 20181113\n",
            "Processed and saved data for 20181114\n",
            "Processed and saved data for 20181115\n",
            "Processed and saved data for 20181116\n",
            "Processed and saved data for 20181117\n",
            "Processed and saved data for 20181118\n",
            "Processed and saved data for 20181119\n",
            "Processed and saved data for 20181120\n",
            "Processed and saved data for 20181121\n",
            "Processed and saved data for 20181122\n",
            "Processed and saved data for 20181123\n",
            "Processed and saved data for 20181124\n",
            "Processed and saved data for 20181125\n",
            "Processed and saved data for 20181126\n",
            "Processed and saved data for 20181127\n",
            "Processed and saved data for 20181128\n",
            "Processed and saved data for 20181129\n",
            "Processed and saved data for 20181130\n",
            "Processed and saved data for 20181201\n",
            "Processed and saved data for 20181202\n",
            "Processed and saved data for 20181203\n",
            "Processed and saved data for 20181204\n",
            "Processed and saved data for 20181205\n",
            "Processed and saved data for 20181206\n",
            "Processed and saved data for 20181207\n",
            "Processed and saved data for 20181208\n",
            "Processed and saved data for 20181209\n",
            "Processed and saved data for 20181210\n",
            "Processed and saved data for 20181211\n",
            "Processed and saved data for 20181212\n",
            "Processed and saved data for 20181213\n",
            "Processed and saved data for 20181214\n",
            "Processed and saved data for 20181215\n",
            "Processed and saved data for 20181216\n",
            "Processed and saved data for 20181217\n",
            "Processed and saved data for 20181218\n",
            "Processed and saved data for 20181219\n",
            "Processed and saved data for 20181220\n",
            "Processed and saved data for 20181221\n",
            "Processed and saved data for 20181222\n",
            "Processed and saved data for 20181223\n",
            "Processed and saved data for 20181224\n",
            "Processed and saved data for 20181225\n",
            "Processed and saved data for 20181226\n",
            "Processed and saved data for 20181227\n",
            "Processed and saved data for 20181228\n",
            "Processed and saved data for 20181229\n",
            "Processed and saved data for 20181230\n",
            "Processed and saved data for 20181231\n"
          ]
        }
      ],
      "source": [
        "get_account_time_value('20180101', '20181231')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kegsq77H7lxk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def process_output_files(start_date, end_date):\n",
        "    base_url = \"http://blockdata.loyce.club\"\n",
        "    download_path = \"/content/drive/MyDrive/bitcoin\"  # Replace with the path to your directory\n",
        "    # Convert date strings to datetime objects\n",
        "    start_date = datetime.strptime(start_date, '%Y%m%d')\n",
        "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
        "\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        date_str = current_date.strftime('%Y%m%d')\n",
        "\n",
        "        # Define URLs for the input transactions and transactions data\n",
        "        output_url = f\"{base_url}/outputs/blockchair_bitcoin_outputs_{date_str}.tsv.gz\"\n",
        "\n",
        "        # Download the data\n",
        "        df_output = pd.read_csv(output_url, compression='gzip', sep='\\t')\n",
        "\n",
        "        # Define the save paths and ensure the directories exist\n",
        "        output_folder = os.path.join(download_path, 'outputs')\n",
        "        # merged_folder = os.path.join(download_path, 'merged')\n",
        "\n",
        "        # if not os.path.exists(trx_folder):\n",
        "        #         os.makedirs(trx_folder)\n",
        "\n",
        "        for folder in [output_folder]:\n",
        "            if not os.path.exists(folder):\n",
        "                os.makedirs(folder)\n",
        "\n",
        "        df_output.to_csv(os.path.join(output_folder, f\"outputs_{date_str}.tsv.gz\"), index=False, sep='\\t', compression='gzip')\n",
        "        # df_merged.to_csv(os.path.join(merged_folder, f\"merged_{date_str}.tsv.gz\"), index=False, sep='\\t', compression='gzip')\n",
        "\n",
        "        print(f\"Processed and saved data for {date_str}\")\n",
        "\n",
        "        # Move to the next date\n",
        "        current_date += timedelta(days=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNsGY_gb8XBd",
        "outputId": "842c54ad-634c-43e3-cc03-e2a597fbd303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed and saved data for 20200101\n",
            "Processed and saved data for 20200102\n",
            "Processed and saved data for 20200103\n",
            "Processed and saved data for 20200104\n",
            "Processed and saved data for 20200105\n",
            "Processed and saved data for 20200106\n",
            "Processed and saved data for 20200107\n"
          ]
        }
      ],
      "source": [
        "process_output_files('20200101', '20200107')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TgEs1gkJaAXMC8e56qPf79mbfsI6gw0Z",
      "authorship_tag": "ABX9TyNR4eydOGndJcIsXXU6P31n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}